{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jun Pan 07/11//2019\n",
    "INTRODUCE OF PROJECT\n",
    "DATABASE\n",
    "MovieLens is a web-based recommender system and virtual community that recommends movies for its users to watch, based on their film preferences using collaborative filtering of members' movie ratings and movie reviews. It contains about 11 million ratings for about 10000 movies.[1] MovieLens was created in 1997 by GroupLens Research, a research lab in the Department of Computer Science and Engineering at the University of Minnesota in order to gather research data on personalized recommendations. In this study, we are going to use a 100k database originally from movieLens, which inclusing movies, ratings, links and tags four files.  In this dataset, we have 100,004 ratings which were rated by 671 users on 9066 movies.\n",
    "\n",
    "The posters of movies can get it from www.themoviedb.org.  We need get api key in advance in order to fetch the posters. \n",
    "\n",
    "ALTERNATING LEAST SQUARES ALGORITHM\n",
    "Collaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix. 'spark.mllib' currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. 'spark.mllib' uses the alternating least squares (ALS) algorithm to learn these latent factors [2]. When using collaborative filtering, getting recommendations is not as simple as predicting for the new entries using a previously generated model. Instead, we need to train again the model but including the new user preferences in order to compare them with other users in the dataset. That is, the recommender needs to be trained every time we have new user ratings (although a single model can be used by multiple users). This makes the process expensive, and it is one of the reasons why scalability is a problem (and Spark a solution!). \n",
    "\n",
    "SPARK\n",
    "Spark is a platform for cluster computing. Spark lets you spread data and computations over clusters with multiple nodes (think of each node as a separate computer). Splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data.\n",
    "\n",
    "As each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computation are performed in parallel over the nodes in the cluster. It is a fact that parallel computation can make certain types of programming tasks much faster [3].\n",
    "\n",
    "Spark will be used locally in our study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-84ee08891836>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;33m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local[4]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"building recommender\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# create a spark session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    171\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m                     \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mJVM\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \"\"\"\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_launch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36m_launch_gateway\u001b[1;34m(conf, insecure)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "#Set Working Environment\n",
    "import pyspark as ps\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrameNaFunctions as DFna\n",
    "from pyspark.sql.functions import udf, col, when\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys, requests, json\n",
    "\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[4]\") \\\n",
    "            .appName(\"building recommender\") \\\n",
    "            .getOrCreate() # create a spark session\n",
    "            \n",
    "sc = spark.sparkContext # create a spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8c1f7eee3ad9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# read movies CSV using spark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#reference: https://stackoverflow.com/questions/28782940/load-csv-file-with-spark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m movies_df = spark.read.csv('C:/Users/tbao/Desktop/data612 final project/movies/movies.csv',\n\u001b[0m\u001b[0;32m      4\u001b[0m                          header=True, quote='\"', sep=\",\", inferSchema=True) \n\u001b[0;32m      5\u001b[0m \u001b[0mmovies_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# read movies CSV using spark \n",
    "#reference: https://stackoverflow.com/questions/28782940/load-csv-file-with-spark\n",
    "movies_df = spark.read.csv('C:/Users/tbao/Desktop/data612 final project/movies/movies.csv',\n",
    "                         header=True, quote='\"', sep=\",\", inferSchema=True) \n",
    "movies_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#View Schema\n",
    "movies_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|     31|   2.5|1260759144|\n",
      "|     1|   1029|   3.0|1260759179|\n",
      "|     1|   1061|   3.0|1260759182|\n",
      "|     1|   1129|   2.0|1260759185|\n",
      "|     1|   1172|   4.0|1260759205|\n",
      "|     1|   1263|   2.0|1260759151|\n",
      "|     1|   1287|   2.0|1260759187|\n",
      "|     1|   1293|   2.0|1260759148|\n",
      "|     1|   1339|   3.5|1260759125|\n",
      "|     1|   1343|   2.0|1260759131|\n",
      "|     1|   1371|   2.5|1260759135|\n",
      "|     1|   1405|   1.0|1260759203|\n",
      "|     1|   1953|   4.0|1260759191|\n",
      "|     1|   2105|   4.0|1260759139|\n",
      "|     1|   2150|   3.0|1260759194|\n",
      "|     1|   2193|   2.0|1260759198|\n",
      "|     1|   2294|   2.0|1260759108|\n",
      "|     1|   2455|   2.5|1260759113|\n",
      "|     1|   2968|   1.0|1260759200|\n",
      "|     1|   3671|   3.0|1260759117|\n",
      "+------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read ratings CSV\n",
    "ratings_df = spark.read.csv('C:/Users/tbao/Desktop/data612 final project/movies/ratings.csv',\n",
    "                         header=True, quote='\"', sep=\",\", inferSchema=True)\n",
    "ratings_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|     31|   2.5|1260759144|\n",
      "|     1|   1029|   3.0|1260759179|\n",
      "|     1|   1061|   3.0|1260759182|\n",
      "|     1|   1129|   2.0|1260759185|\n",
      "|     1|   1172|   4.0|1260759205|\n",
      "|     1|   1263|   2.0|1260759151|\n",
      "|     1|   1287|   2.0|1260759187|\n",
      "|     1|   1293|   2.0|1260759148|\n",
      "|     1|   1339|   3.5|1260759125|\n",
      "|     1|   1343|   2.0|1260759131|\n",
      "|     1|   1371|   2.5|1260759135|\n",
      "|     1|   1405|   1.0|1260759203|\n",
      "|     1|   1953|   4.0|1260759191|\n",
      "|     1|   2105|   4.0|1260759139|\n",
      "|     1|   2150|   3.0|1260759194|\n",
      "|     1|   2193|   2.0|1260759198|\n",
      "|     1|   2294|   2.0|1260759108|\n",
      "|     1|   2455|   2.5|1260759113|\n",
      "|     1|   2968|   1.0|1260759200|\n",
      "|     1|   3671|   3.0|1260759117|\n",
      "+------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter to show only \"userId\" less than 100\n",
    "ratings_df.filter(col(\"userId\") < 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100004\n",
      "671\n",
      "9066\n"
     ]
    }
   ],
   "source": [
    "# Count the number of distinct userIds and distinct movieIds\n",
    "total_count = ratings_df.count()\n",
    "num_users = ratings_df.select(\"userId\").distinct().count()\n",
    "num_movies = ratings_df.select(\"movieId\").distinct().count()\n",
    "print(total_count)\n",
    "print(num_users)\n",
    "print(num_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So total ratings in this dataset are 100,004; total number of unique users are 671; total unique movies in the dataset are 9066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|userId|count|\n",
      "+------+-----+\n",
      "|   148|  132|\n",
      "|   463|  483|\n",
      "|   471|  216|\n",
      "|   496|  126|\n",
      "|   243|  307|\n",
      "|   392|   25|\n",
      "|   540|   20|\n",
      "|   623|  103|\n",
      "|    31|   69|\n",
      "|   516|  149|\n",
      "|    85|  107|\n",
      "|   137|   80|\n",
      "|   251|  119|\n",
      "|   451|   52|\n",
      "|   580|  922|\n",
      "|    65|   27|\n",
      "|   458|   76|\n",
      "|    53|   46|\n",
      "|   255|  145|\n",
      "|   481|  436|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group data by userId, count movies (total numbers of movies rated by each user)\n",
    "ratings_df.groupBy(\"userId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#View Schema\n",
    "ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|movieId|imdbId|tmdbId|\n",
      "+-------+------+------+\n",
      "|      1|114709|   862|\n",
      "|      2|113497|  8844|\n",
      "|      3|113228| 15602|\n",
      "|      4|114885| 31357|\n",
      "|      5|113041| 11862|\n",
      "|      6|113277|   949|\n",
      "|      7|114319| 11860|\n",
      "|      8|112302| 45325|\n",
      "|      9|114576|  9091|\n",
      "|     10|113189|   710|\n",
      "|     11|112346|  9087|\n",
      "|     12|112896| 12110|\n",
      "|     13|112453| 21032|\n",
      "|     14|113987| 10858|\n",
      "|     15|112760|  1408|\n",
      "|     16|112641|   524|\n",
      "|     17|114388|  4584|\n",
      "|     18|113101|     5|\n",
      "|     19|112281|  9273|\n",
      "|     20|113845| 11517|\n",
      "+-------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read links CSV\n",
    "links_df = spark.read.csv('C:/Users/tbao/Desktop/data612 final project/movies/links.csv',\n",
    "                         header=True, quote='\"', sep=\",\", inferSchema=True)\n",
    "links_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- imdbId: integer (nullable = true)\n",
      " |-- tmdbId: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#View Schema\n",
    "links_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+----------+\n",
      "|userId|movieId|                 tag| timestamp|\n",
      "+------+-------+--------------------+----------+\n",
      "|    15|    339|sandra 'boring' b...|1138537770|\n",
      "|    15|   1955|             dentist|1193435061|\n",
      "|    15|   7478|            Cambodia|1170560997|\n",
      "|    15|  32892|             Russian|1170626366|\n",
      "|    15|  34162|         forgettable|1141391765|\n",
      "|    15|  35957|               short|1141391873|\n",
      "|    15|  37729|          dull story|1141391806|\n",
      "|    15|  45950|          powerpoint|1169616291|\n",
      "|    15| 100365|            activist|1425876220|\n",
      "|    15| 100365|         documentary|1425876220|\n",
      "|    15| 100365|              uganda|1425876220|\n",
      "|    23|    150|          Ron Howard|1148672905|\n",
      "|    68|   2174|               music|1249808064|\n",
      "|    68|   2174|               weird|1249808102|\n",
      "|    68|   8623|        Steve Martin|1249808497|\n",
      "|    73| 107999|              action|1430799184|\n",
      "|    73| 107999|               anime|1430799184|\n",
      "|    73| 107999|             kung fu|1430799184|\n",
      "|    73| 111624|               drama|1431584497|\n",
      "|    73| 111624|               indie|1431584497|\n",
      "+------+-------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read links CSV\n",
    "tags_df = spark.read.csv('C:/Users/tbao/Desktop/data612 final project/movies/tags.csv',\n",
    "                         header=True, quote='\"', sep=\",\", inferSchema=True)\n",
    "tags_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- tag: string (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#View Schema\n",
    "tags_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After review all the dataset, we are going to build ALS model to train the data.  \n",
    "Firstly, we are going to get and parse movies and ratings data into Spark RDDs. Resilient Distributed Datasets (RDD) is a fundamental data structure of Spark. It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster. RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes [4]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datafram before \n",
    "ratings = ratings_df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset train (60%), validation (20%), test (20%)\n",
    "training_df, validation_df, test_df = ratings_df.randomSplit([.6, .2, .2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: int, movieId: int, rating: double]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we are going to build an ALS based recommender system. ALS recommender is a matrix factorization algorithm that uses Alternating Least Squares with Weighted-Lamda-Regularization (ALS-WR). It makes the regularization parameter less dependent on the scale of the dataset. This way the best parameter learned from the sampled subset can be aplied to the full dataset and we will get similar performance. The latent factors should explain the observed user to item ratings and map new users to optimal movie recommendations. Accuracy of ALS recommendation system will be checked by using RMSE.\n",
    "We set the following parameters for alternating least squares model. We overiew the ranks and pick the best model. RMSE are used for error checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "iterations = 10\n",
    "regularization_parameter = 0.1\n",
    "ranks = range(4, 12)\n",
    "errors = []\n",
    "err = 0\n",
    "tolerance = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.9296717041877965\n",
      "5 0.933994757073167\n",
      "6 0.9388801740102636\n",
      "7 0.9321476793716605\n",
      "8 0.9378321733349055\n",
      "9 0.9405846005982855\n",
      "10 0.9398717918372941\n",
      "11 0.9422854359647153\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1\n",
    "\n",
    "for rank in ranks:\n",
    "    als = ALS(maxIter=iterations, regParam=regularization_parameter, rank=rank, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\n",
    "    model = als.fit(training_df)\n",
    "    predictions = model.transform(validation_df)\n",
    "    new_predictions = predictions.filter(col('prediction') != np.nan)\n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "    rmse = evaluator.evaluate(new_predictions)\n",
    "    errors.append(rmse)\n",
    "\n",
    "    print(rank, rmse)\n",
    "    if rmse < min_error:\n",
    "        min_error = rmse\n",
    "        best_rank = rank\n",
    "print(best_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that rank 4 is the best, then we choose it as final model and check the RMSE value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9296717041877965\n"
     ]
    }
   ],
   "source": [
    "final_als = ALS(maxIter=10, \n",
    "                regParam=0.1, \n",
    "                rank=4, \n",
    "                userCol=\"userId\", \n",
    "                itemCol=\"movieId\", \n",
    "                ratingCol=\"rating\")\n",
    "final_model = final_als.fit(training_df)\n",
    "final_pred = final_model.transform(validation_df)\n",
    "final_pred = final_pred.filter(col('prediction') != np.nan)\n",
    "rmse = evaluator.evaluate(final_pred)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take random users by user_ids who have already rated some movies. Then, we get all of their ratings and sort to show the top 10 rated movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "user_id = np.random.choice(userId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=102, movieId=2240, rating=1.0, timestamp=957894741),\n",
       " Row(userId=102, movieId=1888, rating=1.0, timestamp=957895013),\n",
       " Row(userId=102, movieId=37, rating=1.0, timestamp=959976593),\n",
       " Row(userId=102, movieId=3354, rating=1.0, timestamp=957893451),\n",
       " Row(userId=102, movieId=3484, rating=1.0, timestamp=957893428),\n",
       " Row(userId=102, movieId=3511, rating=1.0, timestamp=957893428),\n",
       " Row(userId=102, movieId=2541, rating=1.0, timestamp=957894760),\n",
       " Row(userId=102, movieId=2548, rating=1.0, timestamp=957980378),\n",
       " Row(userId=102, movieId=1584, rating=2.0, timestamp=957894285),\n",
       " Row(userId=102, movieId=590, rating=2.0, timestamp=957894532)]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Showing 10 movie ratings of a random selected user\n",
    "new_user_ratings = ratings_df.filter(ratings_df.userId == user_id)\n",
    "new_user_ratings.sort('rating', ascending=True).take(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            rating|\n",
      "+-------+------------------+\n",
      "|  count|               678|\n",
      "|   mean|3.9749262536873156|\n",
      "| stddev|0.8529848074277973|\n",
      "|    min|               1.0|\n",
      "|    max|               5.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Statistical analysis \n",
    "new_user_ratings.describe('rating').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fc9dd50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_user_ratings.toPandas()['rating'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Unrated Movies\n",
    "Get a list of movieIds with more than 25 users given rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_rated_movieIds = [i.movieId for i in new_user_ratings.select('movieId').distinct().collect()]\n",
    "movieIds = [i.movieId for i in movies_counts.filter(movies_counts.counts > 25).select('movieId').distinct().collect()]\n",
    "new_user_unrated_movieIds = list(set(movieIds) - set(new_user_rated_movieIds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now take all the unrated movie id's for this user and create a spark dataframe with 0 for the predicted rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "num_ratings = len(new_user_unrated_movieIds)\n",
    "cols = ('userId', 'movieId', 'timestamp')\n",
    "timestamps = [int(time.time())] * num_ratings\n",
    "userIds = [user_id] * num_ratings\n",
    "# ratings = [0] * num_ratings\n",
    "new_user_preds = sqlContext.createDataFrame(zip(userIds, new_user_unrated_movieIds, timestamps), cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_preds = final_model.transform(new_user_preds).filter(col('prediction') != np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=102, movieId=306, timestamp=1494286154, prediction=4.91982889175415),\n",
       " Row(userId=102, movieId=44555, timestamp=1494286154, prediction=4.77939510345459),\n",
       " Row(userId=102, movieId=319, timestamp=1494286154, prediction=4.753486156463623),\n",
       " Row(userId=102, movieId=6016, timestamp=1494286154, prediction=4.67483377456665),\n",
       " Row(userId=102, movieId=1060, timestamp=1494286154, prediction=4.66062593460083),\n",
       " Row(userId=102, movieId=27773, timestamp=1494286154, prediction=4.652323246002197),\n",
       " Row(userId=102, movieId=56782, timestamp=1494286154, prediction=4.606232166290283),\n",
       " Row(userId=102, movieId=4226, timestamp=1494286154, prediction=4.60533332824707),\n",
       " Row(userId=102, movieId=69481, timestamp=1494286154, prediction=4.570173740386963),\n",
       " Row(userId=102, movieId=3000, timestamp=1494286154, prediction=4.560894012451172)]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user_preds.sort('prediction', ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can search for movie posters by id in the movie database and then grab links to the image files from www.themoviedb.org. The API key are stored in environment vairable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base url filepath structure. w185 corresponds to size of movie poster.\n",
    "#reference: https://www.johannesbader.ch/2013/11/tutorial-download-posters-with-the-movie-database-api-in-python/\n",
    "api_key = os.environ['API_Key_themoviedb']\n",
    "\n",
    "headers = {'Accept': 'application/json'}\n",
    "payload = {'api_key': api_key} \n",
    "response = requests.get(\"http://api.themoviedb.org/3/configuration\", params=payload, headers=headers)\n",
    "response = json.loads(response.text)\n",
    "base_url = response['images']['base_url'] + 'w185'\n",
    "\n",
    "def get_poster(tmdb_id, base_url):\n",
    "    \n",
    "    # Query themoviedb.org API for movie poster path.\n",
    "    movie_url = 'http://api.themoviedb.org/3/movie/{:}/images'.format(tmdb_id)\n",
    "    headers = {'Accept': 'application/json'}\n",
    "    payload = {'api_key': api_key} \n",
    "    response = requests.get(movie_url, params=payload, headers=headers)\n",
    "    file_path = json.loads(response.text)['posters'][0]['file_path']\n",
    "    return base_url + file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a pipeline to go directly from the TheMovieDB id to the url to displaying the movie poster. With this machinery in hand lets first get the top rated movies the user has already rated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_ratings = new_user_ratings.sort('rating', ascending=False).join(links_df, new_user_ratings.movieId == links_df.movieId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "posters = tuple(Image(url=get_poster(movie.tmdbId, base_url)) for movie in new_user_ratings.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://image.tmdb.org/t/p/w185/nGcgVnsL5yLKqg2X4OZcPetsHdd.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://image.tmdb.org/t/p/w185/yzK39sdkGoiSyKZEWT6HQzaC0BX.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://image.tmdb.org/t/p/w185/vWGGFgy36ZoOQJC9uA6FmaAw2ar.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://image.tmdb.org/t/p/w185/5ek49TUeqStPmKwNdxrCSUAdXNe.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://image.tmdb.org/t/p/w185/btTdmkgIvOi0FFip1sPuZI2oQG6.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://image.tmdb.org/t/p/w185/jQLoOPwNxyILwDbdhn9DsfQz0Sg.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://image.tmdb.org/t/p/w185/ha3niQHexpnQgFgK8SNnrtpctv.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://image.tmdb.org/t/p/w185/jvxLLHwpK52ZJUusd2RJv8oLHUJ.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://image.tmdb.org/t/p/w185/wbVGRBYPRRahIZNGXY9TfHDUSc2.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://image.tmdb.org/t/p/w185/fdCWRMe2MLX2wzKdtItXFOJj5FJ.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(*posters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, now let's see what top recommendations are for this user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_preds = new_user_preds.sort('prediction', ascending=False).join(links_df, new_user_preds.movieId == links_df.movieId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "posters = tuple(Image(url=get_poster(movie.tmdbId, base_url)) for movie in new_user_preds.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://image.tmdb.org/t/p/w185/77CFEssoKesi4zvtADEpIrSKhA3.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://image.tmdb.org/t/p/w185/iJ8EJyjBcb8wkXEEjH0Z3y5SlO7.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://image.tmdb.org/t/p/w185/qak9QuEiH5ENwqxyVEO0onegCx6.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://image.tmdb.org/t/p/w185/spSSfbuAarmqjiDCg1B5RxCxNno.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://image.tmdb.org/t/p/w185/h1nLEgXCNK6sQgup22SYgNTrwJm.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://image.tmdb.org/t/p/w185/rIZX6X0MIHYEebk6W4LABT9VP2c.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://image.tmdb.org/t/p/w185/bFgqkuAFBHNuq0oTtewHwuLVWSO.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://image.tmdb.org/t/p/w185/fQMSaP88cf1nz4qwuNEEFtazuDM.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://image.tmdb.org/t/p/w185/uduCNFTuEcbvhvK8rrvNdf3ahzW.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://image.tmdb.org/t/p/w185/gzlJkVfWV5VEG5xK25cvFGJgkDz.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(*posters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "References:\n",
    "1.http://license.umn.edu/technologies/z05173_movielens-database\n",
    "2.https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html\n",
    "3.https://www.tutorialspoint.com/apache_spark/\n",
    "4.https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
